{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a65d386d",
   "metadata": {},
   "source": [
    "USER - USER CF - we can apply covarriance correlation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1fe0c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            userName  Arizona State University  \\\n",
      "0         ! Superman                         0   \n",
      "1            !mpulse                         1   \n",
      "2               $4R4                         1   \n",
      "3             $aumil                         0   \n",
      "4            (:)_(:)                         0   \n",
      "...              ...                       ...   \n",
      "13384       zulu.sud                         0   \n",
      "13385       zuperman                         0   \n",
      "13386    zzkaustavzz                         0   \n",
      "13387          zztop                         1   \n",
      "13388  ~rattlesnake~                         0   \n",
      "\n",
      "       California Institute of Technology  Carnegie Mellon University  \\\n",
      "0                                       1                           0   \n",
      "1                                       0                           0   \n",
      "2                                       0                           0   \n",
      "3                                       0                           1   \n",
      "4                                       0                           0   \n",
      "...                                   ...                         ...   \n",
      "13384                                   0                           0   \n",
      "13385                                   0                           0   \n",
      "13386                                   0                           0   \n",
      "13387                                   0                           0   \n",
      "13388                                   0                           0   \n",
      "\n",
      "       Clemson University  Columbia University  Cornell University  \\\n",
      "0                       0                    0                   1   \n",
      "1                       0                    0                   0   \n",
      "2                       0                    0                   0   \n",
      "3                       0                    1                   0   \n",
      "4                       0                    0                   0   \n",
      "...                   ...                  ...                 ...   \n",
      "13384                   0                    0                   0   \n",
      "13385                   1                    0                   0   \n",
      "13386                   0                    0                   0   \n",
      "13387                   0                    1                   0   \n",
      "13388                   0                    0                   0   \n",
      "\n",
      "       George Mason University  Georgia Institute of Technology  \\\n",
      "0                            0                                1   \n",
      "1                            0                                0   \n",
      "2                            0                                1   \n",
      "3                            0                                1   \n",
      "4                            0                                0   \n",
      "...                        ...                              ...   \n",
      "13384                        0                                0   \n",
      "13385                        1                                0   \n",
      "13386                        0                                0   \n",
      "13387                        0                                0   \n",
      "13388                        0                                0   \n",
      "\n",
      "       Harvard University  ...  toeflScore  toeflEssay  internExp     greV  \\\n",
      "0                       0  ...       118.0         NaN        2.0  165.000   \n",
      "1                       0  ...        95.0         NaN        0.0  140.000   \n",
      "2                       0  ...       109.0         NaN        3.0  145.000   \n",
      "3                       0  ...       109.0         NaN        0.0  129.625   \n",
      "4                       0  ...       103.0         NaN        0.0  150.000   \n",
      "...                   ...  ...         ...         ...        ...      ...   \n",
      "13384                   0  ...        94.0         NaN        0.0  116.875   \n",
      "13385                   0  ...       101.0         NaN        0.0  159.000   \n",
      "13386                   0  ...       103.0         NaN        4.0  150.000   \n",
      "13387                   0  ...       114.0         NaN        0.0  131.750   \n",
      "13388                   0  ...       109.0         NaN       12.0  151.000   \n",
      "\n",
      "          greQ  journalPubs  confPubs    cgpa  \\\n",
      "0      169.000            2         0  0.9100   \n",
      "1      166.000            0         0  0.8280   \n",
      "2      168.000            0         0  0.8970   \n",
      "3      159.375            0         0  0.8600   \n",
      "4      166.000            0         0  0.0764   \n",
      "...        ...          ...       ...     ...   \n",
      "13384  155.125            0         0  0.7430   \n",
      "13385  165.000            0         0  0.4930   \n",
      "13386  155.000            0         0     inf   \n",
      "13387  167.875            0         0  0.9160   \n",
      "13388  151.000            0         0  0.6420   \n",
      "\n",
      "                                                univName  admit  \n",
      "0      Virginia Polytechnic Institute and State Unive...      0  \n",
      "1                    University of Minnesota Twin Cities      1  \n",
      "2                      University of Southern California      1  \n",
      "3                      University of Southern California      1  \n",
      "4                                     University of Utah      1  \n",
      "...                                                  ...    ...  \n",
      "13384                         University of Texas Dallas      1  \n",
      "13385             University of North Carolina Charlotte      0  \n",
      "13386                     University of Illinois Chicago      0  \n",
      "13387                   University of Michigan Ann Arbor      1  \n",
      "13388                         University of Pennsylvania      0  \n",
      "\n",
      "[13389 rows x 112 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load user data into a pandas DataFrame\n",
    "user_data = pd.read_csv(\"user_user_with_toefl_score.csv\")\n",
    "\n",
    "# Apply one-hot encoding on 'univName' and 'major' columns\n",
    "one_hot_univ = pd.get_dummies(user_data['univName'])\n",
    "one_hot_major = pd.get_dummies(user_data['major'])\n",
    "\n",
    "# Concatenate the original DataFrame with the one-hot encoded DataFrames\n",
    "user_data_encoded = pd.concat([user_data, one_hot_univ, one_hot_major], axis=1)\n",
    "\n",
    "# Define aggregation functions for grouping by 'userName'\n",
    "agg_funcs = {col: 'max' for col in one_hot_univ.columns}  # Take max for one-hot encoded university columns\n",
    "agg_funcs.update({col: 'max' for col in one_hot_major.columns})  # Update for one-hot encoded major columns\n",
    "for col in user_data.columns:\n",
    "    if col not in ['userName'] and col not in one_hot_univ.columns and col not in one_hot_major.columns:\n",
    "        agg_funcs[col] = 'first'  # Take the first value for other non-encoded columns\n",
    "\n",
    "# Group by 'userName' and apply aggregation functions\n",
    "user_combined_data = user_data_encoded.groupby('userName').agg(agg_funcs).reset_index()\n",
    "\n",
    "# Print the combined DataFrame\n",
    "print(user_combined_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c720c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['userName', 'Arizona State University',\n",
      "       'California Institute of Technology', 'Carnegie Mellon University',\n",
      "       'Clemson University', 'Columbia University', 'Cornell University',\n",
      "       'George Mason University', 'Georgia Institute of Technology',\n",
      "       'Harvard University',\n",
      "       ...\n",
      "       'toeflScore', 'toeflEssay', 'internExp', 'greV', 'greQ', 'journalPubs',\n",
      "       'confPubs', 'cgpa', 'univName', 'admit'],\n",
      "      dtype='object', length=112)\n"
     ]
    }
   ],
   "source": [
    "print(user_combined_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b07e2302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      userName  Arizona State University  California Institute of Technology  \\\n",
      "2281  aditya57                         0                                   0   \n",
      "\n",
      "      Carnegie Mellon University  Clemson University  Columbia University  \\\n",
      "2281                           0                   0                    0   \n",
      "\n",
      "      Cornell University  George Mason University  \\\n",
      "2281                   0                        0   \n",
      "\n",
      "      Georgia Institute of Technology  Harvard University  ...  researchExp  \\\n",
      "2281                                0                   0  ...            0   \n",
      "\n",
      "      industryExp  toeflScore  toeflEssay  internExp   greV   greQ  \\\n",
      "2281           35       110.0        28.0        3.0  163.0  166.0   \n",
      "\n",
      "      journalPubs  confPubs   cgpa  \n",
      "2281            0         0  0.823  \n",
      "\n",
      "[1 rows x 109 columns]\n"
     ]
    }
   ],
   "source": [
    "user_data_new = user_combined_data.drop(columns=['univName', 'admit', 'major'])\n",
    "print(user_data_new[user_data_new['userName'] == \"aditya57\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffb69173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "user_data_new.fillna(0, inplace=True)  # Replace NaN values with 0\n",
    "user_data_new.replace([np.inf, -np.inf], 0, inplace=True)  # Replace infinite values with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cd4c625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['143saf' 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 18 112.0 26.0\n",
      " 5.0 160.0 167.0 0 0 0.85]\n",
      "['AB25' 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 66 94.0 21.0\n",
      " 0.0 146.0 157.0 0 0 0.7828]\n"
     ]
    }
   ],
   "source": [
    "print(user_data_new[user_data_new['userName'] == '143saf'].values.flatten())\n",
    "print(user_data_new[user_data_new['userName'] == 'AB25'].values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31cb3352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['143saf' 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 18 112.0 26.0\n",
      " 5.0 160.0 167.0 0 0 0.85 26]\n",
      "['AB25' 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 66 94.0 21.0\n",
      " 0.0 146.0 157.0 0 0 0.7828 60]\n"
     ]
    }
   ],
   "source": [
    "user_data_new['userid'] = [i for i, _ in enumerate(user_data_new['userName'], start=1)]\n",
    "print(user_data_new[user_data_new['userName'] == '143saf'].values.flatten())\n",
    "print(user_data_new[user_data_new['userName'] == 'AB25'].values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e957f3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00 -2.63882639e-02 -7.65335016e-02 ...  4.34292786e-02\n",
      "   3.93741356e-03  3.92448634e-03]\n",
      " [-2.63882639e-02  1.00000000e+00 -4.15912394e-03 ...  5.44762246e-03\n",
      "   4.02962477e-04 -4.42586470e-04]\n",
      " [-7.65335016e-02 -4.15912394e-03  1.00000000e+00 ...  6.06010026e-03\n",
      "   8.36839497e-03 -5.04369540e-04]\n",
      " ...\n",
      " [ 4.34292786e-02  5.44762246e-03  6.06010026e-03 ...  1.00000000e+00\n",
      "   9.54172715e-03  1.10650037e-02]\n",
      " [ 3.93741356e-03  4.02962477e-04  8.36839497e-03 ...  9.54172715e-03\n",
      "   1.00000000e+00  8.51191053e-03]\n",
      " [ 3.92448634e-03 -4.42586470e-04 -5.04369540e-04 ...  1.10650037e-02\n",
      "   8.51191053e-03  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for user1 in user_data_new['userName']:\n",
    "#     for user2 in user_data_new['userName']:\n",
    "# #         print(data_user1.shape, data_user2.shape,  user1)\n",
    "#         if user1 != user2:\n",
    "# #             print(user1, user2, user_data_new[user_data_new['userName']== user1].shape,user_data_new[user_data_new['userName']== user2].shape )\n",
    "# #             break\n",
    "#             data_user1 = user_data_new.loc[user_data_new['userName'] == user1, user_data_new.columns != 'userName'].values.flatten()\n",
    "#             data_user2 = user_data_new.loc[user_data_new['userName'] == user2, user_data_new.columns != 'userName'].values.flatten()\n",
    "# #             print(data_user1, user1, data_user2)\n",
    "\n",
    "#             correlation_coefficient, _ = pearsonr(data_user1, data_user2)\n",
    "#             correlation_matrix.loc[user1, user2] = correlation_coefficient\n",
    "\n",
    "# # Filter out NaN values (non-matching users) from the correlation matrix\n",
    "# correlation_matrix = correlation_matrix.dropna()\n",
    "\n",
    "user_data_without_name = user_data_new.drop('userName', axis=1)\n",
    "\n",
    "# Calculate the correlation matrix using np.corrcoef()\n",
    "correlation_matrix = np.corrcoef(user_data_without_name, rowvar=False)\n",
    "\n",
    "# Display or further process the correlation matrix\n",
    "print(correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2c1fa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ID for '143saf': 26\n",
      "Similar users for 143saf : [ 1.14912691e-03  4.00494623e-02  1.12636793e-02 -1.15847357e-02\n",
      "  1.63447780e-02  3.84285191e-02 -1.59087613e-02  2.62527289e-02\n",
      " -3.36987151e-03 -3.60099570e-03  1.36417260e-02 -1.73242915e-02\n",
      "  1.63447780e-02  1.00293128e-02  5.26776262e-04  2.11627481e-02\n",
      "  3.83245031e-02  7.63312030e-03  3.27635206e-02  2.84731964e-02\n",
      " -9.74422516e-03  4.26052391e-02  4.34708230e-02 -7.11289171e-03\n",
      "  1.28076648e-02 -4.85665985e-03  1.00000000e+00  8.59971494e-02\n",
      "  6.25543593e-02  6.49526773e-02  4.94947942e-02  3.83233220e-02\n",
      " -2.04474788e-03  6.66645558e-03  9.78725969e-03  2.66061022e-03\n",
      "  2.39867549e-02  8.15969739e-03  2.87894439e-02  1.48818197e-02\n",
      "  1.40985085e-02  4.13849945e-02 -1.73006092e-02  2.22865227e-02\n",
      "  2.01112456e-02 -1.68276737e-02  2.31198915e-02 -8.05314906e-04\n",
      "  1.57482471e-04  2.01719455e-02  9.15539696e-03  2.90383753e-02\n",
      " -9.97222748e-03 -9.09442665e-03 -7.31426759e-04 -4.70205137e-03\n",
      "  2.39731510e-02  9.11099364e-03  3.12035835e-02  9.95926744e-03\n",
      " -2.35589949e-03 -6.00549499e-03 -1.94472376e-03  2.34930737e-03\n",
      " -4.70394364e-03 -7.65763569e-03 -1.00687991e-02 -2.86330479e-03\n",
      " -7.93781882e-04 -1.87377214e-02 -6.36524921e-03 -3.69139675e-02\n",
      " -1.77521546e-03  3.47101158e-02  1.90106888e-02  1.60579680e-02\n",
      "  5.34152557e-02 -1.37497326e-03 -1.12261903e-03 -1.12261903e-03\n",
      "  4.06682842e-02 -2.97150288e-03 -1.12261903e-03 -1.37497326e-03\n",
      " -2.24574138e-03 -4.56538911e-03 -7.93781882e-04 -7.93781882e-04\n",
      " -7.93781882e-04 -7.93781882e-04 -4.49551613e-03 -1.12261903e-03\n",
      " -7.93781882e-04 -7.93781882e-04 -3.64028564e-03 -2.24574138e-03\n",
      "  2.24018722e-02 -1.12261903e-03  6.76189082e-03  2.32929495e-02\n",
      "  1.96854190e-02  1.52456592e-02  1.79509514e-02  2.50690326e-02\n",
      "  2.23966062e-02 -3.03610796e-03  1.06750431e-02  3.90066595e-03\n",
      " -1.33497216e-02]\n"
     ]
    }
   ],
   "source": [
    "# Perform user-user collaborative filtering for a target user\n",
    "target_user_id = user_data_new.loc[user_data_new['userName'] == '143saf', 'userid'].iloc[0]\n",
    "print(\"User ID for '143saf':\", target_user_id)\n",
    "similar_users = correlation_matrix[target_user_id]\n",
    "print(\"Similar users for\", target_user, \":\", similar_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1297be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1987frank', '198921', '19arjun89', '1990', 'ARK']\n"
     ]
    }
   ],
   "source": [
    "top_k_indices = np.argsort(similar_users)[::-1][:5]\n",
    "top_k_users = [user_data_new.iloc[i]['userName'] for i in top_k_indices]\n",
    "print(top_k_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a85228d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Obtaining dependency information for sentence-transformers from https://files.pythonhosted.org/packages/76/2c/bd95032aeb087b0706596af0a4518c4bfe0439a1bb149048ece18b617766/sentence_transformers-2.7.0-py3-none-any.whl.metadata\n",
      "  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting transformers<5.0.0,>=4.34.0 (from sentence-transformers)\n",
      "  Obtaining dependency information for transformers<5.0.0,>=4.34.0 from https://files.pythonhosted.org/packages/09/c8/844d5518a6aeb4ffdc0cf0cae65ae13dbe5838306728c5c640b5a6e2a0c9/transformers-4.40.0-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: numpy in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (0.17.3)\n",
      "Requirement already satisfied: Pillow in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: filelock in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.12.1)\n",
      "Requirement already satisfied: requests in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.0)\n",
      "Requirement already satisfied: sympy in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Collecting huggingface-hub>=0.15.1 (from sentence-transformers)\n",
      "  Obtaining dependency information for huggingface-hub>=0.15.1 from https://files.pythonhosted.org/packages/05/c0/779afbad8e75565c09ffa24a88b5dd7e293c92b74eb09df6435fc58ac986/huggingface_hub-0.22.2-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2022.7.9)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers<5.0.0,>=4.34.0->sentence-transformers)\n",
      "  Obtaining dependency information for tokenizers<0.20,>=0.19 from https://files.pythonhosted.org/packages/90/79/d17a0f491d10817cd30f1121a07aa09c8e97a81114b116e473baf1577f09/tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/shwetimasakshi/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.17.3\n",
      "    Uninstalling huggingface-hub-0.17.3:\n",
      "      Successfully uninstalled huggingface-hub-0.17.3\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.2\n",
      "    Uninstalling tokenizers-0.13.2:\n",
      "      Successfully uninstalled tokenizers-0.13.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.32.1\n",
      "    Uninstalling transformers-4.32.1:\n",
      "      Successfully uninstalled transformers-4.32.1\n",
      "Successfully installed huggingface-hub-0.22.2 sentence-transformers-2.7.0 tokenizers-0.19.1 transformers-4.40.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3134bf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "applicant_profiles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load BERT model\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "# Sample dataset (replace this with your actual dataset)\n",
    "# data = {\n",
    "#     'userName': ['143saf', 'AB25', 'abhijitgang', 'agteakash', 'alankarMIS'],\n",
    "#     'major': ['Robotics', 'MIS', 'MIS', 'MIS', 'MIS'],\n",
    "#     'researchExp': [0, 0, 0, 0, 0],\n",
    "#     'industryExp': [18, 66, 0, 0, 0],\n",
    "#     'internExp': [5.0, 0.0, 0.0, 0.0, 0.0],\n",
    "#     'greV': [160.0, 146.0, 89.25, 150.0, 147.0],\n",
    "#     'greQ': [167.0, 157.0, 163.625, 161.0, 156.0],\n",
    "#     'journalPubs': [0, 0, 0, 0, 0],\n",
    "#     'confPubs': [0, 0, 0, 0, 0],\n",
    "#     'cgpa': [0.85, 0.7828, 0.57, 0.622, 0.52],\n",
    "#     'univName': ['MIT', 'Stanford', 'Harvard', 'Caltech', 'Carnegie Mellon'],\n",
    "#     'admit': [1, 1, 1, 1, 1]\n",
    "# }\n",
    "\n",
    "data = pd.read_csv('/Users/shwetimasakshi/Desktop/user_user_wo_toefl_score.csv')\n",
    "df = pd.DataFrame(data)\n",
    "print('data loaded')\n",
    "\n",
    "# Function to generate BERT embeddings for textual data\n",
    "def generate_bert_embeddings(texts):\n",
    "    return model.encode(texts)\n",
    "\n",
    "# Calculate BERT embeddings for applicant profiles\n",
    "applicant_profiles = generate_bert_embeddings(df['userName'])\n",
    "print('applicant_profiles')\n",
    "# Calculate similarity matrix between applicant profiles\n",
    "similarity_matrix = cosine_similarity(applicant_profiles)\n",
    "\n",
    "# Recommend universities for each applicant based on similar applicants' admissions\n",
    "recommendations = {}\n",
    "for i, user in enumerate(df['userName']):\n",
    "    similar_users_indices = np.argsort(similarity_matrix[i])[::-1][1:]  # Exclude self\n",
    "    similar_users_admissions = df.iloc[similar_users_indices]['univName'].unique()\n",
    "    recommendations[user] = similar_users_admissions\n",
    "\n",
    "# Print recommendations for each applicant\n",
    "for user, univs in recommendations.items():\n",
    "    print(f\"Recommendations for {user}: {', '.join(univs)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
